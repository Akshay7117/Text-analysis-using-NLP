{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hbf_BX7z0xh5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEckl4Cd00zN",
        "outputId": "f2330a0e-33e9-46e7-ac6a-e0ddbd99e33c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Function to extract article title and text\n",
        "def extract_article_content(url):\n",
        "    try:\n",
        "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
        "        response = requests.get(url, headers=headers)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Extracting article title\n",
        "        title = soup.find('title').get_text(strip=True) if soup.find('title') else 'No Title Found'\n",
        "\n",
        "        # Extracting article text\n",
        "        article_text = []\n",
        "        for paragraph in soup.find_all('p'):\n",
        "            article_text.append(paragraph.get_text(strip=True))\n",
        "\n",
        "        # Joining paragraphs to form complete article text\n",
        "        article_text = '\\n\\n'.join(article_text)\n",
        "\n",
        "        return title, article_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting content from {url}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Load the Excel file\n",
        "excel_file = '/content/drive/MyDrive/Input.xlsx'  # Replace with your actual file path\n",
        "df = pd.read_excel(excel_file)\n",
        "\n",
        "# Create a directory to save the articles\n",
        "output_dir = 'extracted_articles'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Loop through each URL and extract content\n",
        "for index, row in df.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "    try:\n",
        "        title, content = extract_article_content(url)\n",
        "        if content:\n",
        "            # Cleaning filename for safe usage\n",
        "            filename = re.sub(r'[^\\w\\s-]', '', str(url_id).strip())\n",
        "            filename = filename.replace(\" \", \"_\") + \".txt\"\n",
        "\n",
        "            # Save the content to a text file\n",
        "            with open(os.path.join(output_dir, filename), 'w', encoding='utf-8') as file:\n",
        "                file.write(f\"Title: {title}\\n\\n{content}\")\n",
        "\n",
        "            print(f\"Article {url_id} saved successfully.\")\n",
        "        else:\n",
        "            print(f\"Skipping {url_id}: No content extracted.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to extract or save article {url_id}: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KGTezsS1EGy",
        "outputId": "c7b1112d-54c1-429e-8aca-2860d1fd89ec"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article blackassign0001 saved successfully.\n",
            "Article blackassign0002 saved successfully.\n",
            "Article blackassign0003 saved successfully.\n",
            "Article blackassign0004 saved successfully.\n",
            "Article blackassign0005 saved successfully.\n",
            "Article blackassign0006 saved successfully.\n",
            "Article blackassign0007 saved successfully.\n",
            "Article blackassign0008 saved successfully.\n",
            "Article blackassign0009 saved successfully.\n",
            "Article blackassign0010 saved successfully.\n",
            "Article blackassign0011 saved successfully.\n",
            "Article blackassign0012 saved successfully.\n",
            "Article blackassign0013 saved successfully.\n",
            "Article blackassign0014 saved successfully.\n",
            "Article blackassign0015 saved successfully.\n",
            "Article blackassign0016 saved successfully.\n",
            "Article blackassign0017 saved successfully.\n",
            "Article blackassign0018 saved successfully.\n",
            "Article blackassign0019 saved successfully.\n",
            "Article blackassign0020 saved successfully.\n",
            "Article blackassign0021 saved successfully.\n",
            "Article blackassign0022 saved successfully.\n",
            "Article blackassign0023 saved successfully.\n",
            "Article blackassign0024 saved successfully.\n",
            "Article blackassign0025 saved successfully.\n",
            "Article blackassign0026 saved successfully.\n",
            "Article blackassign0027 saved successfully.\n",
            "Article blackassign0028 saved successfully.\n",
            "Article blackassign0029 saved successfully.\n",
            "Article blackassign0030 saved successfully.\n",
            "Article blackassign0031 saved successfully.\n",
            "Article blackassign0032 saved successfully.\n",
            "Article blackassign0033 saved successfully.\n",
            "Article blackassign0034 saved successfully.\n",
            "Article blackassign0035 saved successfully.\n",
            "Article blackassign0036 saved successfully.\n",
            "Article blackassign0037 saved successfully.\n",
            "Article blackassign0038 saved successfully.\n",
            "Article blackassign0039 saved successfully.\n",
            "Article blackassign0040 saved successfully.\n",
            "Article blackassign0041 saved successfully.\n",
            "Article blackassign0042 saved successfully.\n",
            "Article blackassign0043 saved successfully.\n",
            "Article blackassign0044 saved successfully.\n",
            "Article blackassign0045 saved successfully.\n",
            "Article blackassign0046 saved successfully.\n",
            "Article blackassign0047 saved successfully.\n",
            "Article blackassign0048 saved successfully.\n",
            "Article blackassign0049 saved successfully.\n",
            "Article blackassign0050 saved successfully.\n",
            "Article blackassign0051 saved successfully.\n",
            "Article blackassign0052 saved successfully.\n",
            "Article blackassign0053 saved successfully.\n",
            "Article blackassign0054 saved successfully.\n",
            "Article blackassign0055 saved successfully.\n",
            "Article blackassign0056 saved successfully.\n",
            "Article blackassign0057 saved successfully.\n",
            "Article blackassign0058 saved successfully.\n",
            "Article blackassign0059 saved successfully.\n",
            "Article blackassign0060 saved successfully.\n",
            "Article blackassign0061 saved successfully.\n",
            "Article blackassign0062 saved successfully.\n",
            "Article blackassign0063 saved successfully.\n",
            "Article blackassign0064 saved successfully.\n",
            "Article blackassign0065 saved successfully.\n",
            "Article blackassign0066 saved successfully.\n",
            "Article blackassign0067 saved successfully.\n",
            "Article blackassign0068 saved successfully.\n",
            "Article blackassign0069 saved successfully.\n",
            "Article blackassign0070 saved successfully.\n",
            "Article blackassign0071 saved successfully.\n",
            "Article blackassign0072 saved successfully.\n",
            "Article blackassign0073 saved successfully.\n",
            "Article blackassign0074 saved successfully.\n",
            "Article blackassign0075 saved successfully.\n",
            "Article blackassign0076 saved successfully.\n",
            "Article blackassign0077 saved successfully.\n",
            "Article blackassign0078 saved successfully.\n",
            "Article blackassign0079 saved successfully.\n",
            "Article blackassign0080 saved successfully.\n",
            "Article blackassign0081 saved successfully.\n",
            "Article blackassign0082 saved successfully.\n",
            "Article blackassign0083 saved successfully.\n",
            "Article blackassign0084 saved successfully.\n",
            "Article blackassign0085 saved successfully.\n",
            "Article blackassign0086 saved successfully.\n",
            "Article blackassign0087 saved successfully.\n",
            "Article blackassign0088 saved successfully.\n",
            "Article blackassign0089 saved successfully.\n",
            "Article blackassign0090 saved successfully.\n",
            "Article blackassign0091 saved successfully.\n",
            "Article blackassign0092 saved successfully.\n",
            "Article blackassign0093 saved successfully.\n",
            "Article blackassign0094 saved successfully.\n",
            "Article blackassign0095 saved successfully.\n",
            "Article blackassign0096 saved successfully.\n",
            "Article blackassign0097 saved successfully.\n",
            "Article blackassign0098 saved successfully.\n",
            "Article blackassign0099 saved successfully.\n",
            "Article blackassign0100 saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install textstat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4GHNZDqFH9Q",
        "outputId": "23bef78b-b2ed-4621-cf69-ed29f3a4c0bb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textstat\n",
            "  Downloading textstat-0.7.3-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.15.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.15.0 textstat-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from textblob import TextBlob\n",
        "from nltk.tag import pos_tag\n",
        "from textstat import flesch_reading_ease, syllable_count, lexicon_count\n",
        "# Function to clean text using stop words list\n",
        "def clean_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = word_tokenize(text.lower())\n",
        "    cleaned_text = [word for word in word_tokens if word.isalnum() and word not in stop_words]\n",
        "    return ' '.join(cleaned_text)\n",
        "# Function to calculate derived variables\n",
        "def calculate_derived_variables(text):\n",
        "    blob = TextBlob(text)\n",
        "    positive_score = sum(1 for sentence in blob.sentences if sentence.sentiment.polarity > 0)\n",
        "    negative_score = sum(1 for sentence in blob.sentences if sentence.sentiment.polarity < 0)\n",
        "    polarity_score = blob.sentiment.polarity\n",
        "    subjectivity_score = blob.sentiment.subjectivity\n",
        "\n",
        "    sentences = sent_tokenize(text)\n",
        "    avg_sentence_length = sum(len(word_tokenize(sentence)) for sentence in sentences) / len(sentences)\n",
        "    complex_word_count = sum(1 for word, pos in pos_tag(word_tokenize(text)) if lexicon_count(word) > 2)\n",
        "    word_count = lexicon_count(text)\n",
        "    syllable_count_per_word = sum(syllable_count(word) for word in word_tokenize(text)) / word_count\n",
        "    personal_pronouns = sum(1 for word, pos in pos_tag(word_tokenize(text)) if pos == 'PRP')\n",
        "\n",
        "    avg_word_length = sum(len(word) for word in word_tokenize(text)) / word_count\n",
        "\n",
        "    return (positive_score, negative_score, polarity_score, subjectivity_score,\n",
        "            avg_sentence_length, complex_word_count, word_count,\n",
        "            syllable_count_per_word, personal_pronouns, avg_word_length)\n",
        "\n",
        "# Function to compute Fog Index\n",
        "def fog_index(avg_sentence_length, percentage_complex_words):\n",
        "    return 0.4 * (avg_sentence_length + percentage_complex_words)"
      ],
      "metadata": {
        "id": "uB2yknK4EdTw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "# Create a directory to save the articles\n",
        "output_dir = 'extracted_articles'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Define output columns\n",
        "output_columns = [\n",
        "    'URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE',\n",
        "    'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE',\n",
        "    'COMPLEX WORD COUNT', 'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH'\n",
        "]\n",
        "\n",
        "output_data = []\n",
        "\n",
        "# Loop through each URL and extract content\n",
        "for index, row in df.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "    try:\n",
        "        title, content = extract_article_content(url)\n",
        "        if content:\n",
        "            cleaned_content = clean_text(content)\n",
        "\n",
        "            # Calculate derived variables\n",
        "            (positive_score, negative_score, polarity_score, subjectivity_score,\n",
        "             avg_sentence_length, complex_word_count, word_count,\n",
        "             syllable_per_word, personal_pronouns, avg_word_length) = calculate_derived_variables(cleaned_content)\n",
        "\n",
        "            percentage_complex_words = complex_word_count / word_count * 100\n",
        "            fog_index_value = fog_index(avg_sentence_length, percentage_complex_words)\n",
        "\n",
        "            # Save analysis results to output data\n",
        "            output_data.append({\n",
        "                'URL_ID': url_id,\n",
        "                'URL': url,\n",
        "                'POSITIVE SCORE': positive_score,\n",
        "                'NEGATIVE SCORE': negative_score,\n",
        "                'POLARITY SCORE': polarity_score,\n",
        "                'SUBJECTIVITY SCORE': subjectivity_score,\n",
        "                'AVG SENTENCE LENGTH': avg_sentence_length,\n",
        "                'PERCENTAGE OF COMPLEX WORDS': percentage_complex_words,\n",
        "                'FOG INDEX': fog_index_value,\n",
        "                'AVG NUMBER OF WORDS PER SENTENCE': avg_sentence_length,\n",
        "                'COMPLEX WORD COUNT': complex_word_count,\n",
        "                'WORD COUNT': word_count,\n",
        "                'SYLLABLE PER WORD': syllable_per_word,\n",
        "                'PERSONAL PRONOUNS': personal_pronouns,\n",
        "                'AVG WORD LENGTH': avg_word_length\n",
        "            })\n",
        "\n",
        "            print(f\"Analysis completed for {url_id}.\")\n",
        "\n",
        "        else:\n",
        "            print(f\"Skipping {url_id}: No content extracted.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to process article {url_id}: {e}\")\n",
        "# Save output data to Excel\n",
        "output_excel_file = '/content/drive/MyDrive/Output Data Structure.xlsx'  # Replace with desired output file path\n",
        "output_df = pd.DataFrame(output_data, columns=output_columns)\n",
        "output_df.to_excel(output_excel_file, index=False)\n",
        "\n",
        "print(\"Analysis results saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2aiPHg5EdXV",
        "outputId": "68142f7b-6c87-4886-e348-db9f96831cfd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analysis completed for blackassign0001.\n",
            "Analysis completed for blackassign0002.\n",
            "Analysis completed for blackassign0003.\n",
            "Analysis completed for blackassign0004.\n",
            "Analysis completed for blackassign0005.\n",
            "Analysis completed for blackassign0006.\n",
            "Analysis completed for blackassign0007.\n",
            "Analysis completed for blackassign0008.\n",
            "Analysis completed for blackassign0009.\n",
            "Analysis completed for blackassign0010.\n",
            "Analysis completed for blackassign0011.\n",
            "Analysis completed for blackassign0012.\n",
            "Analysis completed for blackassign0013.\n",
            "Analysis completed for blackassign0014.\n",
            "Analysis completed for blackassign0015.\n",
            "Analysis completed for blackassign0016.\n",
            "Analysis completed for blackassign0017.\n",
            "Analysis completed for blackassign0018.\n",
            "Analysis completed for blackassign0019.\n",
            "Analysis completed for blackassign0020.\n",
            "Analysis completed for blackassign0021.\n",
            "Analysis completed for blackassign0022.\n",
            "Analysis completed for blackassign0023.\n",
            "Analysis completed for blackassign0024.\n",
            "Analysis completed for blackassign0025.\n",
            "Analysis completed for blackassign0026.\n",
            "Analysis completed for blackassign0027.\n",
            "Analysis completed for blackassign0028.\n",
            "Analysis completed for blackassign0029.\n",
            "Analysis completed for blackassign0030.\n",
            "Analysis completed for blackassign0031.\n",
            "Analysis completed for blackassign0032.\n",
            "Analysis completed for blackassign0033.\n",
            "Analysis completed for blackassign0034.\n",
            "Analysis completed for blackassign0035.\n",
            "Analysis completed for blackassign0036.\n",
            "Analysis completed for blackassign0037.\n",
            "Analysis completed for blackassign0038.\n",
            "Analysis completed for blackassign0039.\n",
            "Analysis completed for blackassign0040.\n",
            "Analysis completed for blackassign0041.\n",
            "Analysis completed for blackassign0042.\n",
            "Analysis completed for blackassign0043.\n",
            "Analysis completed for blackassign0044.\n",
            "Analysis completed for blackassign0045.\n",
            "Analysis completed for blackassign0046.\n",
            "Analysis completed for blackassign0047.\n",
            "Analysis completed for blackassign0048.\n",
            "Analysis completed for blackassign0049.\n",
            "Analysis completed for blackassign0050.\n",
            "Analysis completed for blackassign0051.\n",
            "Analysis completed for blackassign0052.\n",
            "Analysis completed for blackassign0053.\n",
            "Analysis completed for blackassign0054.\n",
            "Analysis completed for blackassign0055.\n",
            "Analysis completed for blackassign0056.\n",
            "Analysis completed for blackassign0057.\n",
            "Analysis completed for blackassign0058.\n",
            "Analysis completed for blackassign0059.\n",
            "Analysis completed for blackassign0060.\n",
            "Analysis completed for blackassign0061.\n",
            "Analysis completed for blackassign0062.\n",
            "Analysis completed for blackassign0063.\n",
            "Analysis completed for blackassign0064.\n",
            "Analysis completed for blackassign0065.\n",
            "Analysis completed for blackassign0066.\n",
            "Analysis completed for blackassign0067.\n",
            "Analysis completed for blackassign0068.\n",
            "Analysis completed for blackassign0069.\n",
            "Analysis completed for blackassign0070.\n",
            "Analysis completed for blackassign0071.\n",
            "Analysis completed for blackassign0072.\n",
            "Analysis completed for blackassign0073.\n",
            "Analysis completed for blackassign0074.\n",
            "Analysis completed for blackassign0075.\n",
            "Analysis completed for blackassign0076.\n",
            "Analysis completed for blackassign0077.\n",
            "Analysis completed for blackassign0078.\n",
            "Analysis completed for blackassign0079.\n",
            "Analysis completed for blackassign0080.\n",
            "Analysis completed for blackassign0081.\n",
            "Analysis completed for blackassign0082.\n",
            "Analysis completed for blackassign0083.\n",
            "Analysis completed for blackassign0084.\n",
            "Analysis completed for blackassign0085.\n",
            "Analysis completed for blackassign0086.\n",
            "Analysis completed for blackassign0087.\n",
            "Analysis completed for blackassign0088.\n",
            "Analysis completed for blackassign0089.\n",
            "Analysis completed for blackassign0090.\n",
            "Analysis completed for blackassign0091.\n",
            "Analysis completed for blackassign0092.\n",
            "Analysis completed for blackassign0093.\n",
            "Analysis completed for blackassign0094.\n",
            "Analysis completed for blackassign0095.\n",
            "Analysis completed for blackassign0096.\n",
            "Analysis completed for blackassign0097.\n",
            "Analysis completed for blackassign0098.\n",
            "Analysis completed for blackassign0099.\n",
            "Analysis completed for blackassign0100.\n",
            "Analysis results saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rIbTGkgHBfPm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}